import torchaudio
import torch
from torchaudio.transforms import Spectrogram
import matplotlib.pyplot as plt
import os
import torch.nn as nn
import torch.optim
import pypesq
class SpeechEnhancementCRNN(nn.Module):
    def __init__(self, num_samples, hidden_size=64):
        super().__init__()

        self.num_samples = num_samples
        self.hidden_size = hidden_size

        self.conv1 = nn.Conv1d(in_channels=201, out_channels=128, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.stride1 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=3, stride=2, padding=1)

        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.stride2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1)

        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)
        self.relu3 = nn.ReLU()
        self.stride3 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1)

        self.fc1 = nn.Linear(in_features=128 * 15, out_features=256)
        self.relu4 = nn.ReLU()
        self.dropout = nn.Dropout(p=0.5)

        self.conv4 = nn.Conv1d(in_channels=1, out_channels=hidden_size, kernel_size=1)

        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True)

        self.fc2 = nn.Linear(in_features=num_samples * hidden_size, out_features=num_samples)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.stride1(x)

        x = self.conv2(x)
        x = self.relu2(x)
        x = self.stride2(x)

        x = self.conv3(x)
        x = self.relu3(x)
        x = self.stride3(x)

        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.relu4(x)
        x = self.dropout(x)

        x = x.unsqueeze(1)
        x = self.conv4(x)
        x = x.transpose(1, 2).contiguous()
        x = x.view(-1, self.num_samples, self.hidden_size)

        x, _ = self.lstm(x)

        x = x.reshape(x.shape[0], -1)
        x = self.fc2(x)

        return x


def wav_to_spectrogram(file_path, n_fft=400, hop_length=200, n_mels=128):
    waveform, sample_rate = torchaudio.load(file_path)

    transform = nn.Sequential(
        Spectrogram(n_fft=n_fft, hop_length=hop_length),
        torchaudio.transforms.AmplitudeToDB(),
    )

    spectrogram = transform(waveform)

    return spectrogram


# Define the directory path
dir_path = 'C:\\Users\\LENOVO\\Desktop\\10dB'

# Get a list of all the WAV files in the directory
file_names = [f for f in os.listdir(dir_path) if f.endswith('.wav')]

# Iterate over the file names and load each WAV file
spectrograms = []
for file_name in file_names:
    file_path = os.path.join(dir_path, file_name)
    spectrogram = wav_to_spectrogram(file_path)
    spectrograms.append(spectrogram)


def plot_spectrogram(spectrogram, title=None):
    plt.figure(figsize=(10, 5))
    plt.imshow(spectrogram.log2()[0, :, :].detach().numpy(), cmap='gray_r', aspect='auto')
    plt.title(title if title else "Spectrogram")
    plt.xlabel("Time")
    plt.ylabel("Frequency")
    plt.colorbar(format='%+2.0f dB')
    plt.show()


# Visualize all the spectrograms
for i, spectrogram in enumerate(spectrograms):
    plot_spectrogram(spectrogram, title=f"Spectrogram {i + 1}")

print("File names:", file_names)
print("Number of spectrograms:", len(spectrograms))


def prepare_data_for_cnn1d(spectrograms, clean_speech_files, resize_shape=(128, 128)):
    data = []
    for spectrogram, clean_speech_file in zip(spectrograms, clean_speech_files):
        # Load the clean speech waveform
        clean_waveform, _ = torchaudio.load(clean_speech_file)

        # Transform the clean speech waveform to a spectrogram
        clean_spectrogram = Spectrogram()(clean_waveform)

        # Resize the clean spectrogram to match the desired shape
        resized_clean_spectrogram = torch.nn.functional.interpolate(
            clean_spectrogram.unsqueeze(0), size=resize_shape, mode='bilinear', align_corners=False
        ).squeeze(0)

        # Resize the input spectrogram based on the time dimension of the clean spectrogram
        resized_spectrogram = torch.nn.functional.interpolate(
            spectrogram.unsqueeze(0), size=resized_clean_spectrogram.shape[-2:], mode='bilinear', align_corners=False
        ).squeeze(0)

        # Reshape the tensor to a 3D tensor
        resized_spectrogram = resized_spectrogram.squeeze(0).transpose(1, 0)

        data.append(resized_spectrogram)

    # Concatenate the spectrograms into a single tensor
    data = torch.stack(data, dim=0)

    return data


model = SpeechEnhancementCRNN(num_samples=128, hidden_size=64)


def generate_label(input_spectrogram, clean_speech_files):
    labels = []
    for clean_speech_file in clean_speech_files:
        # Load the clean speech waveform
        clean_waveform, _ = torchaudio.load(clean_speech_file)

        # Transform the clean speech waveform to a spectrogram
        clean_spectrogram = Spectrogram()(clean_waveform)

        # Resize the clean spectrogram to match the input spectrogram size
        resized_clean_spectrogram = torch.nn.functional.interpolate(
            clean_spectrogram.unsqueeze(0), size=input_spectrogram.shape[-2:], mode='bilinear', align_corners=False
        ).squeeze(0)

        labels.append(resized_clean_spectrogram)

    # Concatenate the spectrograms along a new dimension
    labels = torch.stack(labels, dim=0)

    labels = labels.unsqueeze(1)

    return labels


# Define the directory path and a list of clean speech files
clean_dir_path = 'C:\\Users\\LENOVO\\Desktop\\clean'
clean_speech_files = [
    'C:\\Users\\LENOVO\\Desktop\\clean\\sp01.wav',
    'C:\\Users\\LENOVO\\Desktop\\clean\\sp02.wav',
    'C:\\Users\\LENOVO\\Desktop\\clean\\sp03.wav',
    'C:\\Users\\LENOVO\\Desktop\\clean\\sp04.wav',
    'C:\\Users\\LENOVO\\Desktop\\clean\\sp05.wav',
]

# Modify the function call to include the clean_speech_files argument
cnn_data = prepare_data_for_cnn1d(spectrograms, clean_speech_files)

# Get a list of all the WAV files in the directory
file_names = [f for f in os.listdir(dir_path) if f.endswith('.wav')]

# Iterate over the file names and load each WAV file
spectrograms = []
for file_name in file_names:
    file_path = os.path.join(dir_path, file_name)
    spectrogram = wav_to_spectrogram(file_path)
    spectrograms.append(spectrogram)

# Define the loss function
criterion = nn.MSELoss()

# Define the optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Train the model
for epoch in range(10):
    for spectrogram in spectrograms:
        # Get the input
        inputs = spectrogram

        # Generate the label (e.g., clean speech signal)
        labels = generate_label(inputs, clean_speech_files)

        # Forward pass
        outputs = model(inputs)

        print("Outputs shape:", outputs.shape)
        print("Labels shape:", labels.shape)

        # Reshape labels to match the shape of outputs
        batch_size = outputs.size(0)
        labels = torch.reshape(labels, (batch_size, 1, -1, 1))

        print("Reshaped labels shape:", labels.shape)

        # Calculate the loss
        loss = criterion(outputs, labels)

        # Backpropagate the loss
        optimizer.zero_grad()
        loss.backward()

        # Update the parameters
        optimizer.step()

        # Print the loss
        print("Loss:", loss.item())

# Evaluate the model
pesq_scores = []
for i in range(len(spectrograms)):
    # Get the clean speech signal
    clean_signal = spectrograms[i]

    # Get the enhanced speech signal
    enhanced_signal = model(clean_signal)

    # Calculate the PESQ score
    pesq_score = pypesq.compute(clean_signal, enhanced_signal)

    pesq_scores.append(pesq_score)

# Print the PESQ scores
print(pesq_scores)

# Plot the PESQ scores
plt.plot(pesq_scores)
plt.xlabel("Epoch")
plt.ylabel("PESQ score")
plt.show()

